---
title: '(bigpunch)为什么说大模型训练很难？'
category: '/小书匠/收集/知乎问答/bigpunch/2dfeb16645ea37cf39b54418e71245f5'
slug: 'https://www.zhihu.com/question/498271491/answer/1981120245616906843'
createDate: '2025-12-7 21:57:54'
grammar_mathjax: false
grammar_footnote: false
grammar_ins: false
emoji: 'b'
tags: '人工智能,人工智能算法,深度学习（Deep Learning）,神经网络模型,人工智能框架'

---


[toc]


# 问题

提问者：**<a href="https://www.zhihu.com/people/ZOMII">ZOMI酱</a>**
提问时间: 2021-11-11 23:36:16

自从Bert网络模型产数量超过3亿规模，当时候只是觉得性能好，没想到GPT系列出来后，GPT-3直接用170B规模的参数量模型精度碾压竞品。

接着就是新一轮的竞争了，后面的事情就有点可怕了，Google推出万亿稀疏switch transformer，huawei推出2000亿稠密鹏程盘古大模型，微软推出Turing-NLG有1000亿参数，英伟达推出MegatronLM系列。

大家都说大模型难，大模型训练除了集群调度麻烦，还难在哪里吗？

# 回答

回答者： **<a href="https://www.zhihu.com/people/bigpunch-46">bigpunch</a>**
回答时间: 2025-12-7 21:57:54
点赞总数: 4978
评论总数: 175
收藏总数: 5838
喜欢总数：265

很多人光看参数量在那蹭蹭往上涨，什么175B、万亿参数、MoE架构，外行看热闹，觉得就是堆显卡、烧电费的事情，甚至有些投资人都觉得这就是个大力出奇迹的工程，钱到位了卡到位了，模型自然就出来了。

如果你也这么想，那我只能说，幸亏你没负责这块业务，不然多少预算都能给你烧得连灰都不剩。

咱们先撇开集群调度那个事儿不谈，虽然Kubernetes在那边天天报节点故障也挺烦人的，但那毕竟是运维层面的事，是有固定解法的。

真正让大模型训练变成玄学的，是工程稳定性和算法收敛性之间那个极其脆弱的平衡。

你要明白一个概念，以前我们训练BERT，甚至早期的ResNet，那是小打小闹。几十张卡，甚至几张卡，跑个几天，挂了就重跑，成本可控。

现在呢？

现在的训练是几千张A100或者H800连在一起，要连续跑几个月。

这就像是你开着一辆F1赛车，不仅要跑得快，还要在满负荷状态下连续跑三个月不停车，中间还得一边跑一边换轮胎，稍微哪个零件哆嗦一下，整辆车就散架了。

这里面第一个巨难的点，叫大规模并行下的计算效率崩塌。

大家都知道Megatron-LM好用，英伟达的东西确实强，这里顺便提一嘴，如果你想真搞懂大模型底层训练，NVIDIA的Megatron-LM论文和代码库是你必须要啃下来的硬骨头，别只看网上的解析，去读源码，去看它是怎么做Tensor Parallelism和Pipeline Parallelism切分的。

但是，理论归理论，实际上线你就懵了。

当你把模型切分到几千张卡上的时候，通信开销会变得巨大无比。你以为你的算力是线性增长的？根本不是。卡越多，卡与卡之间为了同步梯度、同步参数，花在通信上的时间就越多。

我们以前遇到过一个Case，两千张卡的集群，看着监控上GPU利用率挺高，一算MFU（Model FLOPs Utilization），才30%不到。这意味啥？意味着你花了一亿买显卡，只有三千万在干正事，剩下七千万都在等数据传输、等同步。

这时候就需要用FlashAttention这类技术了，Tri Dao写的FlashAttention V2的论文建议你们一定要看，这是在算子层面把显存读写做到极致的典范，能极大提升训练效率。这东西现在基本是大模型训练的标配，不加这个，你的训练成本直接翻倍。

这就完了吗？没有。

硬件是会坏的。

这也是很多人没概念的地方。在3000张卡的规模下，硬件故障是常态，不是异常。

IB网卡会掉线，光模块会烧坏，GPU会出现ECC错误，甚至机房空调出问题导致局部过热降频。只要有一张卡坏了，或者慢了，整个训练任务就会卡住，或者速度被拖慢到不可接受的程度（这就是著名的Straggler问题）。

你想象一下，半夜两点，训练群里报警响了，Loss突然变成NaN，或者直接卡死不动了。一群年薪百万的工程师爬起来查日志，最后发现是某台机器的PCIE插槽松了。

这种事情每天都在发生。

所以大模型训练难，难在你要搞一套极强的容错和Checkpoints机制。你不能每一步都存盘，那样IO会把存储打爆，也会拖慢训练；你也不能很久才存一次，不然挂一次你要回滚到两天前，几十万的电费就打水漂了。

怎么在训练不中断的情况下，快速做异步Checkpoint，怎么在节点挂掉后快速热启动替换节点，这都是不仅费脑子还费头发的活儿。

再来说说数据。

大家都说数据是石油。这话不对，原始数据是原油，里面全是沙子和硫磺，直接加到发动机里引擎立刻报废。

现在公认的结论是，模型效果好不好，一半看架构，一半看数据质量。

很多人觉得我有几PB的文本数据，丢进去练就行了。大错特错。

清洗数据能洗脱你一层皮。

你需要做去重，MinHash、SimHash这些算法得在大规模分布式集群上跑。为什么要极致去重？因为大模型有很强的记忆能力，一段话重复出现几次，它就记住了，它就开始复读了，它的泛化能力就废了。

这里推荐大家关注一下RedPajama或者The Pile这些开源数据集的处理流程。特别是RedPajama，他们详细披露了如何复刻LLaMA的数据配方，里面关于数据清洗、质量过滤的细节非常有参考价值。

而且数据不仅要干净，还要配比。

代码数据放多少？维基百科放多少？网络小说放多少？arxiv论文放多少？

这个配比是个玄学。你加多了代码，逻辑推理能力强了，但是文采变差了；你加多了小说，说话好听了，但是开始胡说八道了。

Meta的LLaMA 3的技术报告，前段时间刚出的，建议反复研读。他们花了巨大的篇幅讲数据工程，讲怎么做Up-sampling，怎么清洗Evaluation set，这才是真功夫。比起模型架构那点微创新，数据上的功夫才是拉开差距的关键。

这中间还有个更隐蔽的坑，叫数据污染。

你的测试集，可能早就混在训练集里被模型看过了。这就像考试前透题，模型考分很高，一上线实战就露馅。要从几万亿token里把这几千条测试题找出来剔除掉，这工程量你品品。

接下来咱们聊聊算法调优的黑盒性质。

小模型训练，Loss震荡了，调调学习率，改改Batch Size，半天就有结果。

大模型不一样。你改一个超参，可能要跑两周才知道这个改动对不对。这个反馈周期太长了。

而且大模型训练经常会出现Loss Spike（损失刺刺）。

就是跑着跑着，Loss突然像心电图一样窜上去，然后就下不来了，模型直接崩坏。

原因是什么？可能是数据里混进了一段乱码，可能是梯度爆炸了，可能是BF16精度溢出了。为了查这个问题，你需要把模型停下来，去分析梯度范数，去检查那个时间点喂进去的数据。

这就像在海底捞针。

有时候为了救回来一个模型，我们需要手动回滚到出问题前的Checkpoint，跳过那段坏数据，或者临时调低学习率，小心翼翼地把模型哄过去。这过程跟走钢丝没区别。

这里必须要提一下DeepSpeed。微软的这个库真的是救命稻草，特别是它的ZeRO系列优化（ZeRO-1, 2, 3）。如果你要训练大模型，DeepSpeed的代码和配置一定要烂熟于心。它能帮你把显存压榨到极致，还能提供一些Offload的功能，防止OOM。

还有个大家容易忽视的点：评估难。

模型练出来了，怎么知道它好不好？

看Loss？Loss低不代表生成效果好。  
刷榜单？C-Eval、MMLU这些榜单，现在的模型都在刷，分数虚高得厉害。

真正好用的评估，还得靠人。但是人太慢了，太贵了。

于是大家开始搞Model-as-a-Judge，用GPT-4去评判你练出来的模型。但这里面又有偏差。

所以现在的行业现状是，各家都有一套自己的内部评估集，这个是从实际业务场景里抽出来的，绝对保密。这才是真正的护城河。

如果你对模型评估感兴趣，可以去看看LMSYS Org的Chatbot Arena。这是一种基于Elo积分的竞技场模式，让用户盲测，相对来说是最客观的。研究他们的评估体系，对建立自己的评估标准很有帮助。

最后，我想说一个更现实的问题：人才断层。

能熟练使用PyTorch写个Transformer的人一抓一大把。

但是，真正懂底层CUDA编程，懂NCCL通信原理，懂InfiniBand网络拓扑，同时还懂Transformer架构，懂分布式系统，能对着几千行的Shell脚本和Python混合代码查Bug的人，凤毛麟角。

这种人，是既懂算法又懂工程的复合型人才。

大模型训练，本质上是一个系统工程。它不是单纯的算法问题，也不是单纯的代码问题，它是对一家公司算力、数据、算法、工程基建、资金流的极限压力测试。

你说难不难？

它难在每一个环节都不能有短板。

算力不够，练不动；  
数据不好，练傻了；  
工程不稳，练不成；  
评估不准，不知道练了个啥。

所以，当Google推出Switch Transformer，华为搞盘古，微软搞Turing-NLG的时候，不要只看那些千亿万亿的数字。要看到这些数字背后，是无数次通宵达旦的Debug，是烧掉了几个亿电费换来的血泪经验，是成百上千个顶尖工程师像搭积木一样，小心翼翼把这个摩天大楼搭起来的过程。

对于想入行或者正在坑里的朋友，我的建议是：少看媒体吹牛，多看Technical Report，多读源码，多关注底层系统。

别总盯着ChatGPT又有什么新功能了，去看看PyTorch的最新Release Note，看看CUDA的文档，看看Hugging Face Transformers库里关于分布式训练的源码实现。

这里再夹带个私货，推荐去看看Andrej Karpathy的GitHub和博客。虽然他是OpenAI的前核心人物，但他写的代码和教程非常Grounding，从最基础的micrograd写起，能帮你把对神经网络的理解从抽象拉回到具体的代码实现上。

大模型这碗饭，看着香，吃起来是真的硌牙。但正因为它难，所以壁垒高，所以值得我们去死磕。

  

原文地址：[(bigpunch)为什么说大模型训练很难？](https://www.zhihu.com/question/498271491/answer/1981120245616906843) 


